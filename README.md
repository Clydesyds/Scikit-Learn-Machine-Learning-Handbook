# ü§ñ The Ultimate Scikit-Learn Machine Learning Handbook

### 100% Runnable, Production-Ready Machine Learning Notebooks ‚Äî No External Data Needed

---

## üéØ Project Mission

This project is a **comprehensive, beginner-to-senior level Machine Learning guide** built entirely using **scikit-learn (sklearn)** ‚Äî the most popular ML library in Python.

Every algorithm is implemented using **in-built sklearn datasets**, so you can run every notebook instantly ‚Äî no external data downloads, no setup issues, just clean and working ML code.

Our goal:  
To take you from **basic ML concepts** to **production-ready engineering workflows** that reflect the real work of a **Senior Machine Learning Engineer**.

---

## ‚úÖ Why This Project Exists

### üö´ No More Data Setup Problems  
Every example runs instantly with sklearn‚Äôs built-in datasets like `load_iris`, `load_diabetes`, and `fetch_california_housing`.

### üß© Go Beyond `.fit()` and `.predict()`  
Each notebook explores **hyperparameters**, **regularization**, **bias-variance tradeoff**, and **model interpretability** ‚Äî not just model training.

### üîó Learn Production-Ready Pipelines  
Every workflow includes `Pipeline` and `ColumnTransformer` to show how real-world ML systems prevent data leakage and ensure reproducibility.

---

## üë®‚Äçüíª Who Should Use This Repository?

| **Audience Level** | **Learning Focus** | **Key Takeaway** |
|----------------------|--------------------|------------------|
| **Beginner / Intermediate** | Understanding ML basics, model training, evaluation metrics. | Focus on Sections 1 & 2 |
| **Advanced Learner** | Cross-validation, scaling, hyperparameter tuning, and ensemble methods. | Focus on Sections 3 & 4 |
| **Senior ML Engineer** | Model interpretability (SHAP/LIME), feature importance, and full ML pipelines. | Focus on Section 5 |

---

## üìò Repository Overview

This repository contains **24 Jupyter Notebooks**, grouped into **6 learning modules**.  
Each notebook includes theory, implementation, evaluation, and interpretation.

---

### üìå Module 01 ‚Äî Regression Algorithms
| Notebook | Topic |
|---------|-------|
| üîó [`01_Linear_Regression.ipynb`](Module_01_Regression/01_Linear_Regression.ipynb) | Simple & Multiple Linear Regression |
| üîó [`02_Ridge_Lasso_ElasticNet.ipynb`](Module_01_Regression/02_Ridge_Lasso_ElasticNet.ipynb) | Regularization Techniques |
| üîó [`03_SVR_Support_Vector_Regression.ipynb`](Module_01_Regression/03_SVR_Support_Vector_Regression.ipynb) | Kernel SVR |
| üîó [`04_KNN_Regression.ipynb`](Module_01_Regression/04_KNN_Regression.ipynb) | K-Nearest Neighbor Regression |

---

### üìå Module 02 ‚Äî Classification Algorithms
| Notebook | Topic |
|---------|-------|
| üîó [`05_Logistic_Regression.ipynb`](Module_02_Classification/05_Logistic_Regression.ipynb) | Binary & Multi-class Classification |
| üîó [`06_Decision_Tree_Classifier.ipynb`](Module_02_Classification/06_Decision_Tree_Classifier.ipynb) | Tree Splits & Pruning |
| üîó [`07_KNN_Classifier.ipynb`](Module_02_Classification/07_KNN_Classifier.ipynb) | Distance-based Classification |
| üîó [`08_SVM_Classifier.ipynb`](Module_02_Classification/08_SVM_Classifier.ipynb) | Margin Optimization |
| üîó [`09_Naive_Bayes.ipynb`](Module_02_Classification/09_Naive_Bayes.ipynb) | Probabilistic Classifiers |

---

### üìå Module 03 ‚Äî Ensemble Techniques
| Notebook | Topic |
|---------|-------|
| üîó [`10_Random_Forest.ipynb`](Module_03_Ensemble/10_Random_Forest.ipynb) | Bagging & OOB Scoring |
| üîó [`11_AdaBoost.ipynb`](Module_03_Ensemble/11_AdaBoost.ipynb) | Adaptive Boosting |
| üîó [`12_Gradient_Boosting.ipynb`](Module_03_Ensemble/12_Gradient_Boosting.ipynb) | Residual Learning |
| üîó [`13_XGBoost_LightGBM.ipynb`](Module_03_Ensemble/13_XGBoost_LightGBM.ipynb) | Fast Gradient Boosting |
| üîó [`14_Stacking_Voting_Classifier.ipynb`](Module_03_Ensemble/14_Stacking_Voting_Classifier.ipynb) | Hybrid Model Stacking |

---

### üìå Module 04 ‚Äî Clustering (Unsupervised ML)
| Notebook | Topic |
|---------|-------|
| üîó [`15_KMeans_Clustering.ipynb`](Module_04_Clustering/15_KMeans_Clustering.ipynb) | Cluster Partitioning |
| üîó [`16_Hierarchical_Clustering.ipynb`](Module_04_Clustering/16_Hierarchical_Clustering.ipynb) | Dendrograms |
| üîó [`17_DBSCAN.ipynb`](Module_04_Clustering/17_DBSCAN.ipynb) | Density-Based Clustering |
| üîó [`18_Gaussian_Mixture_Models.ipynb`](Module_04_Clustering/18_Gaussian_Mixture_Models.ipynb) | Soft Clustering |

---

### üìå Module 05 ‚Äî Dimensionality Reduction
| Notebook | Topic |
|---------|-------|
| üîó [`19_PCA.ipynb`](Module_05_Dimensionality_Reduction/19_PCA.ipynb) | Principal Component Analysis |
| üîó [`20_ICA.ipynb`](Module_05_Dimensionality_Reduction/20_ICA.ipynb) | Independent Component Analysis |
| üîó [`21_tSNE_and_UMAP.ipynb`](Module_05_Dimensionality_Reduction/21_tSNE_and_UMAP.ipynb) | Non-linear Visualization |

---

### üìå Module 06 ‚Äî Machine Learning Engineering
| Notebook | Topic |
|---------|-------|
| üîó [`22_Feature_Engineering_and_Preprocessing.ipynb`](Module_06_Engineering_Best_Practices/22_Feature_Engineering_and_Preprocessing.ipynb) | Encoding, Scaling & Transformers |
| üîó [`23_Pipelines_and_ColumnTransformer.ipynb`](Module_06_Engineering_Best_Practices/23_Pipelines_and_ColumnTransformer.ipynb) | Data Leakage Prevention |
| üîó [`24_Model_Selection_and_Tuning.ipynb`](Module_06_Engineering_Best_Practices/24_Model_Selection_and_Tuning.ipynb) | Grid Search + CV |

---

## üî¨ Notebook Format (Consistent Across All Notebooks)

Each notebook follows a **standardized 5-section structure**:

| **Section** | **Focus** | **Skill Level** |
|--------------|------------|----------------|
| **1. Theoretical Foundation** | Intuition, math formula, cost function, optimization concept. | Beginner |
| **2. Setup & Dataset** | Import libraries, load sklearn datasets, and train-test split. | All |
| **3. Preprocessing & Modeling** | Feature scaling, encoding, and model training. | Intermediate |
| **4. Evaluation & Metrics** | Metrics like MSE, R¬≤, ROC-AUC, F1, confusion matrix, ROC curve. | Advanced |
| **5. Interpretation & Next Steps** | SHAP/LIME analysis, regularization, bias-variance, feature importance. | Senior |

---

## üöÄ Getting Started

### Step 1: Clone the Repository
```bash
git clone https://github.com/rohanmistry231/Scikit-Learn-Machine-Learning-Handbook.git
cd Scikit-Learn-Machine-Learning-Handbook
```

### Step 2: Launch Jupyter Notebook

```bash
jupyter notebook
```

Then open any notebook (e.g., [`Module_01_Regression/01_Linear_Regression.ipynb`](Module_01_Regression/01_Linear_Regression.ipynb)).

---

## ü§ù Contributing

We welcome contributions from the ML community!
If you‚Äôd like to add new algorithms, improve explanations, or enhance interpretability sections:

* Follow the **5-section notebook structure**
* Use **only scikit-learn datasets**
* Write **clear, documented, and reproducible code**

---

## üåü Vision

> ‚ÄúTo build the most practical, instantly runnable, and production-focused
> Machine Learning resource for Python and scikit-learn.‚Äù

---

**Author:** Rohan Mistry
**License:** MIT
**Framework:** Scikit-learn, Python 3.9+